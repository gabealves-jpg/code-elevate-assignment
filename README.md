# ğŸ“Š AnÃ¡lise de Logs Web com Apache Spark ğŸš€

Projeto para o Code Elevate

## ğŸ“œ SumÃ¡rio

*   [VisÃ£o Geral do Projeto](#-visÃ£o-geral-do-projeto)
*   [ğŸ¯ Desafio](#-desafio)
*   [ğŸ› ï¸ Tecnologias Utilizadas](#ï¸-tecnologias-utilizadas)
*   [âš™ï¸ ConfiguraÃ§Ã£o do Ambiente (Docker)](#ï¸-configuraÃ§Ã£o-do-ambiente-docker)
*   [â–¶ï¸ Executando o Projeto](#ï¸-executando-o-projeto)
*   [ğŸ“‚ Estrutura do Projeto](#-estrutura-do-projeto)
*   [ğŸ“ Resultados Esperados](#-resultados-esperados)
*   [ğŸ’¡ SeÃ§Ã£o Opcional: Armazenamento dos Logs](#-seÃ§Ã£o-opcional-armazenamento-dos-logs)
*   [ğŸ¤ ContribuiÃ§Ãµes](#-contribuiÃ§Ãµes)
*   [ğŸ‘¨â€ğŸ’» Autor](#-autor)

## ğŸŒ VisÃ£o Geral do Projeto

Este projeto foi desenvolvido como parte de um desafio de Engenharia de Dados. O objetivo principal Ã© analisar um arquivo de log de acesso de servidor web (padrÃ£o Web Server Access Log) para responder a perguntas especÃ­ficas sobre padrÃµes de uso, origens de acesso, endpoints mais visitados, entre outros. A ferramenta central para o processamento dos dados Ã© o Apache Spark, rodando em um ambiente Dockerizado e Databricks, onde construÃ­ as duas soluÃ§Ãµes possÃ­veis, uma para cada caso que falarei mais adiante.

**NÃ£o Ã© preciso rodar ambos, dÃª preferÃªncia ao Docker.**

Ponto importante: Ele utilizar .jars do Maven relacionadas ao S3 da AWS.

## ğŸ¯ Desafio

O projeto visa responder Ã s seguintes questÃµes com base na anÃ¡lise dos logs:

1.  ğŸ¥‡ **Identificar as 10 maiores origens de acesso (Client IP)** por quantidade de acessos.
2.  ğŸšª **Listar os 6 endpoints mais acessados**, desconsiderando aqueles que representam arquivos.
3.  ğŸ‘¤ **Qual a quantidade de Client IPs distintos?**
4.  ğŸ—“ï¸ **Quantos dias de dados estÃ£o representados no arquivo?**
5.  ğŸ’¾ **AnÃ¡lise do tamanho (em bytes) do conteÃºdo das respostas:**
    *   Volume total de dados retornado.
    *   Maior volume de dados em uma Ãºnica resposta.
    *   Menor volume de dados em uma Ãºnica resposta.
    *   Volume mÃ©dio de dados retornado.
6.  ğŸš¨ **Qual o dia da semana com o maior nÃºmero de erros do tipo "HTTP Client Error"?**

## ğŸ› ï¸ Tecnologias Utilizadas

*   ![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)
*   ![Pyspark](https://img.shields.io/badge/Apache_Spark-3.5.0+-orange?logo=apachespark&logoColor=white)
*   ![Docker](https://img.shields.io/badge/Docker-20.x%2B-blue?logo=docker&logoColor=white)
*   ![Docker Compose](https://img.shields.io/badge/Docker_Compose-1.29%2B-blue?logo=docker&logoColor=white)
*   ![Databricks](https://img.shields.io/badge/Databricks-orange) ![AWS](https://img.shields.io/badge/AWS-yellow)




## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente (Docker)

Para rodar este projeto localmente utilizando Docker, siga os passos abaixo:

1.  **Clone o RepositÃ³rio:**
    ```bash
    git clone https://github.com/gabealves-jpg/code-elevate-assignment.git
    cd code-elevate-assignment/docker-solution
    ```

2.  **PrÃ©-requisitos:**
    *   Certifique-se de ter o [Docker](https://docs.docker.com/get-docker/) instalado e em execuÃ§Ã£o na sua mÃ¡quina.

3.  **VariÃ¡veis de Ambiente:**
    *   Para rodar esse script, Ã© necessÃ¡rio setar as variÃ¡veis de ambiente AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY que foram enviadas por e-mail. Abra um terminal e execute:
      ```env
      export AWS_ACCESS_KEY_ID=''
      export AWS_SECRET_ACCESS_KEY='''
      ```
    * OU: Aponte para o access_log que vocÃª possuir no Ã­nicio do script em main_docker.py

## â–¶ï¸ Executando o Projeto (Docker)

No mesmo terminal que foi setada a variÃ¡vel de ambiente e com o Docker e Docker Compose devidamente configurados execute o seguinte comandos:

```bash
docker-compose up --build
```

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente (Databricks)
Caso queira rodar o projeto utilizando o Databricks, siga os passos abaixo:

1.  **No Databricks vocÃª pode: clonar o repositÃ³rio, ou importar os arquivos diretamente na sua workspace;**

2.  **Utilize o arquivo de log no S3 ou configure os caminhos necessÃ¡rios para o notebook 'main_dtb':**
    *   **Caminho do arquivo de log (`access_log`):** VocÃª pode definir uma variÃ¡vel com as keys (conforme enviado por e-mail) OU modificar diretamente a variÃ¡vel `file_path` no inÃ­cio do script `main_dtb.ipynb` para apontar para o seu arquivo de log.
    Databricks:
    <img src="docker-solution/resources/dtb_notebook.png" alt="Detabricks" width="50%" height="auto">
    Docker: 
    <img src="docker-solution/resources/docker.png" alt="Docker" width="50%" height="auto">
    *   **Caminho de saÃ­da no S3:** Ainda assim, vocÃª DEVE definir a variÃ¡vel de ambiente correspondente (conforme enviado por e-mail) para que o notebook possa salvar os resultados no S3. Conforme abaixo:

2. **Sete as variÃ¡veis de ambiente no comeÃ§o do notebook 'main_dtb', conforme enviado por e-mail. OU: Aponte para o access_log que vocÃª possuir no Ã­nicio do script em main_dtb.ipynb (em file_path): Ainda assim, precisarÃ¡ da variÃ¡vel de ambiente para salvar no S3**




3. **Clique para executar tudo. JÃ¡ estÃ¡ em ordem. Os resultados serÃ£o printados na tela**


## â–¶ï¸ Algumas observaÃ§Ãµes:
- Decidi entregar duas soluÃ§Ãµes pois acredito que cada uma serve um caso diferente. A do Databricks Ã© algo mais fixo numa plataforma e prÃ³ximo do que o banco tem hoje, e eu nÃ£o queria depender somente do Databricks. HÃ¡ tambÃ©m outras formas de fazer deploy por lÃ¡ usando um Workflow ou atÃ© um DLT, tudo orquestrado e tudo numa ferramenta sÃ³. Funciona completamente utilizando Unity Catalog  e tendo como storage soluÃ§Ãµes da AWS (S3, etc) e Azure (Blob Storage). Nesse caso, para estudar novos caminhos, utilizei o S3.

- Para a soluÃ§Ã£o com Docker, acredito ser uma soluÃ§Ã£o mais agnÃ³stica de plataforma, por ser mais containerizada. Pensei em algumas soluÃ§Ãµes para armazenamento como:
    - Elasticsearch. Foi uma das minhas primeiras opÃ§Ãµes, porÃ©m acredito que estava adicionando complexidade ao projeto sem necessidade no momento. Seria Ã³timo para ter uma stack de visualizaÃ§Ã£o de dados com o Kibana, mas nÃ£o tive tanto tempo para executar isso.
    - Considerei tambÃ©m um AWS RDS (Postgres) para armazenar os dados, mas no fim, nÃ£o achei que um banco de dados relacional seria o melhor para armazenar essa log, apesar do log ser estruturado e essa soluÃ§Ã£o parecer ser mais fÃ¡cil que Elastic.
    - Por fim, decidi utilizar o S3, que poderia utilizar tanto no Databricks quando no Docker, e eu poderia abre oportunidades de conectar com o AWS, OpenSearch, Athena e mais uma infinidade de outras soluÃ§Ãµes. No fim, acabei de replicando o esquema de Lake do Databricks, mas acho que faz sentido para logs.

- Os resultados das perguntas estÃ£o salvas como .txt no *Databricks*. Pensei em subir para o S3 na soluÃ§Ã£o do Docker, mas como *esse* S3 nÃ£o estarÃ¡ facilmente acessÃ­vel, decidi nÃ£o prosseguir com essa parte.

