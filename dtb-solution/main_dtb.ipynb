{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "%run analysis\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CodeElevate\").getOrCreate() #Não precisamos dos JARs da S3 aqui\n",
    "\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Processa o arquivo de log que já está setado na função. \n",
    "    Utiliza regex para fazer o parsing dos dados dentro do arquivo de log.\n",
    "    Também formata a coluna 'size' para int, que será usado para as próximas questões.\n",
    "    Além disso, salva o dataframe em...{a definir}\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        df (pyspark.sql.DataFrame): DataFrame com os dados processados.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Se o arquivo de log estiver vazio.\n",
    "        FileNotFoundError: Se o arquivo de log não for encontrado.\n",
    "        AnalysisException: Se houver um erro no processamento Spark.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_df = spark.read.text('resources/access_log.txt')\n",
    "        if raw_df.count() == 0:\n",
    "            raise ValueError(\"O arquivo de log está vazio\")\n",
    "\n",
    "        log_pattern = r'^(\\S+).*\\[(.*?)\\].*\"(\\w+)\\s+([^\\s]+)[^\"]*\"\\s+(\\d+)\\s+(\\S+)'\n",
    "\n",
    "        df = raw_df.select(\n",
    "             F.regexp_extract('value', log_pattern, 1).alias('ip'),\n",
    "             F.regexp_extract('value', log_pattern, 2).alias('timestamp'),\n",
    "             F.regexp_extract('value', log_pattern, 3).alias('method'),\n",
    "             F.regexp_extract('value', log_pattern, 4).alias('path'),\n",
    "             F.regexp_extract('value', log_pattern, 5).alias('status'),\n",
    "             F.regexp_extract('value', log_pattern, 6).alias('size'))\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro ao acessar arquivo: {str(e)}\")\n",
    "        return None\n",
    "    except AnalysisException as e:\n",
    "        print(f\"Erro no processamento Spark: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    #Trocando valores \"-\" por 0    \n",
    "    df = df.withColumn('size', F.when(F.col('size') == '-', '0').otherwise(F.col('size')).cast(\"integer\"))\n",
    "    #Cria coluna de dat_mes_carga para particionamento. Para este case, irei simular que o 'timestamp' será o dat_ref_carga.\n",
    "    df = df.withColumn('dat_mes_carga', F.date_format(F.to_timestamp(F.col('timestamp'), 'dd/MMM/yyyy:HH:mm:ss Z'),'MM/yyyy'))\n",
    "    \n",
    "    df.cache()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Salva o dataframe no OpenSearch.\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): DataFrame com os dados processados.\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        Exception: Ao falhar o save de dados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write\\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(\"/mnt/delta/logs_delta\") #Considerei usar saveAsTable mas é melhor deixar mais genérico\n",
    "\n",
    "        print(\"Salvo com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #Processing & Saving\n",
    "    df = process_data()\n",
    "    #Saving\n",
    "    save_data(df)\n",
    "    #Analysis\n",
    "    analyze_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
